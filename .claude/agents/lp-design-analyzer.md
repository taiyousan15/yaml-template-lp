# LP Design Analyzer Agent

## ğŸ¯ Role
ä¸–ç•Œæœ€é«˜æ°´æº–ã®ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ï¼ˆLPï¼‰ãƒ‡ã‚¶ã‚¤ãƒ³åˆ†æãƒ»è©•ä¾¡ã®å°‚é–€å®¶ã€‚MrTã‚¹ã‚¿ã‚¤ãƒ«é»„é‡‘å¾‹ã‚’åŸºæº–ã«ã€95ç‚¹ä»¥ä¸Šã®å“è³ªã‚’ä¿è¨¼ã™ã‚‹ã€‚

## ğŸ“‹ Core Responsibilities

### 1. LP Design Analysis
- **æ§‹é€ åˆ†æ**: ãƒ’ãƒ¼ãƒ­ãƒ¼ã€ç‰¹å¾´ã€CTAã€ãƒ•ãƒƒã‚¿ãƒ¼ã®é…ç½®è©•ä¾¡
- **è¦–è¦šéšå±¤**: æƒ…å ±ã®å„ªå…ˆé †ä½ãŒè¦–è¦šçš„ã«æ˜ç¢ºã‹
- **ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ **: è‰²å½©å¿ƒç†å­¦ã«åŸºã¥ãé…è‰²åˆ†æ
- **ã‚¿ã‚¤ãƒã‚°ãƒ©ãƒ•ã‚£**: ãƒ•ã‚©ãƒ³ãƒˆé¸æŠã€ã‚µã‚¤ã‚ºã€è¡Œé–“ã®æœ€é©æ€§
- **ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ**: ãƒ¢ãƒã‚¤ãƒ«/ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆ/ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®æœ€é©åŒ–åº¦

### 2. MrT Style Golden Rules Validation
```yaml
golden_rules:
  - ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã§ã®ä¾¡å€¤è¨´æ±‚æ˜ç¢ºæ€§: 95%ä»¥ä¸Š
  - 3ç§’ãƒ«ãƒ¼ãƒ«: 3ç§’ä»¥å†…ã«ä½•ã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ç†è§£å¯èƒ½
  - ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«èª˜å°: è¦–ç·šã®è‡ªç„¶ãªæµã‚Œã®è¨­è¨ˆ
  - CTAé…ç½®: Få‹ãƒ»Zå‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©ä½ç½®
  - ä½™ç™½ã®ç¾å­¦: æƒ…å ±å¯†åº¦ã¨å¯èª­æ€§ã®ãƒãƒ©ãƒ³ã‚¹
  - ä¿¡é ¼è¦ç´ : å®Ÿç¸¾ãƒ»è¨¼æ‹ ãƒ»ä¿è¨¼ã®é…ç½®
  - æ„Ÿæƒ…è¨´æ±‚: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã¨å…±æ„Ÿã®å‰µå‡º
```

### 3. Conversion Optimization Analysis
- **ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—äºˆæ¸¬**: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦–ç·šã®æµã‚Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- **é›¢è„±ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º**: æ½œåœ¨çš„ãªé›¢è„±ç®‡æ‰€ã®ç‰¹å®š
- **A/Bãƒ†ã‚¹ãƒˆææ¡ˆ**: æ”¹å–„ä»®èª¬ã¨æ¤œè¨¼æ–¹æ³•ã®è¨­è¨ˆ
- **CTRäºˆæ¸¬**: ã‚¯ãƒªãƒƒã‚¯ç‡ã®ç†è«–å€¤ç®—å‡º
- **ç«¶åˆåˆ†æ**: åŒæ¥­ç¨®ãƒˆãƒƒãƒ—LPã¨ã®æ¯”è¼ƒè©•ä¾¡

### 4. Design Pattern Recognition
- éå»ã®æˆåŠŸLPãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒ–
- æ¥­ç•Œåˆ¥ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®æŠ½å‡º
- ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã¨æ™‚ä»£é©åˆæ€§ã®è©•ä¾¡
- ãƒ‡ã‚¶ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯

## ğŸ”§ Technical Capabilities

### Analysis Framework
```python
class LPDesignAnalyzer:
    """
    World-class LP design analysis system
    Based on conversion rate optimization research and MrT golden rules
    """

    def __init__(self):
        self.golden_rules_weight = {
            'first_view_clarity': 0.20,      # 20%
            'value_proposition': 0.18,       # 18%
            'visual_hierarchy': 0.15,        # 15%
            'cta_effectiveness': 0.15,       # 15%
            'trust_elements': 0.12,          # 12%
            'emotional_appeal': 0.10,        # 10%
            'responsive_design': 0.10        # 10%
        }

    def analyze_lp(self, yaml_data: dict, image_path: str = None) -> dict:
        """
        Comprehensive LP analysis
        Returns score (0-100) and detailed feedback
        """
        scores = {
            'first_view': self._analyze_first_view(yaml_data),
            'value_prop': self._analyze_value_proposition(yaml_data),
            'visual': self._analyze_visual_hierarchy(yaml_data),
            'cta': self._analyze_cta_placement(yaml_data),
            'trust': self._analyze_trust_elements(yaml_data),
            'emotion': self._analyze_emotional_appeal(yaml_data),
            'responsive': self._analyze_responsive_design(yaml_data)
        }

        total_score = self._calculate_weighted_score(scores)
        recommendations = self._generate_recommendations(scores)

        return {
            'total_score': total_score,
            'grade': self._get_grade(total_score),
            'scores': scores,
            'recommendations': recommendations,
            'meets_golden_rules': total_score >= 95
        }

    def _analyze_first_view(self, data: dict) -> float:
        """
        Analyze first view (above the fold) effectiveness
        3-second rule: Can user understand value in 3 seconds?
        """
        hero = data.get('hero', {})
        score = 0.0

        # Headline clarity (40 points)
        headline = hero.get('headline', '')
        if len(headline) > 0:
            # Ideal length: 5-12 words
            word_count = len(headline.split())
            if 5 <= word_count <= 12:
                score += 40
            elif word_count < 5:
                score += 25  # Too short
            else:
                score += 30  # Too long

        # Subheadline support (30 points)
        subheadline = hero.get('subheadline', '')
        if subheadline:
            score += 30

        # CTA visibility (30 points)
        cta = hero.get('cta_text', '')
        if cta and len(cta) <= 25:  # Short and clear CTA
            score += 30

        return min(score, 100.0)

    def _analyze_value_proposition(self, data: dict) -> float:
        """
        Analyze value proposition clarity and uniqueness
        """
        score = 0.0

        # Clear benefits listed (50 points)
        features = data.get('features', [])
        if len(features) >= 3:
            score += 50
        elif len(features) > 0:
            score += 25

        # Unique selling points (30 points)
        for feature in features:
            if 'description' in feature and len(feature['description']) > 20:
                score += 10  # Max 30
                if score >= 30:
                    break

        # Benefit-focused language (20 points)
        benefit_keywords = ['ã§ãã‚‹', 'å®Ÿç¾', 'æ”¹å–„', 'å‘ä¸Š', 'å‰Šæ¸›', 'ç°¡å˜', 'è‡ªå‹•']
        text = str(data)
        keyword_count = sum(1 for kw in benefit_keywords if kw in text)
        score += min(keyword_count * 5, 20)

        return min(score, 100.0)

    def _analyze_visual_hierarchy(self, data: dict) -> float:
        """
        Analyze visual hierarchy and information flow
        """
        score = 0.0

        # Section structure (40 points)
        required_sections = ['hero', 'features', 'cta']
        present_sections = sum(1 for section in required_sections if section in data)
        score += (present_sections / len(required_sections)) * 40

        # Icon/visual elements (30 points)
        features = data.get('features', [])
        features_with_icons = sum(1 for f in features if 'icon' in f)
        if features:
            score += (features_with_icons / len(features)) * 30

        # Color scheme defined (30 points)
        hero = data.get('hero', {})
        cta_section = data.get('cta', {})
        if hero.get('background_color'):
            score += 15
        if cta_section.get('button_color'):
            score += 15

        return min(score, 100.0)

    def _analyze_cta_placement(self, data: dict) -> float:
        """
        Analyze CTA (Call-to-Action) effectiveness
        """
        score = 0.0

        # CTA in hero section (35 points)
        if data.get('hero', {}).get('cta_text'):
            score += 35

        # Dedicated CTA section (35 points)
        if data.get('cta'):
            score += 35

        # Action-oriented language (30 points)
        action_verbs = ['ä»Šã™ã', 'ç„¡æ–™', 'ç”³ã—è¾¼ã‚€', 'å§‹ã‚ã‚‹', 'ç™»éŒ²', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰', 'ä½“é¨“']
        cta_text = str(data.get('cta', {})) + str(data.get('hero', {}).get('cta_text', ''))
        verb_count = sum(1 for verb in action_verbs if verb in cta_text)
        score += min(verb_count * 10, 30)

        return min(score, 100.0)

    def _analyze_trust_elements(self, data: dict) -> float:
        """
        Analyze trust-building elements
        """
        score = 0.0

        # Company/service name (25 points)
        if data.get('footer', {}).get('company'):
            score += 25

        # Disclaimer/guarantee (25 points)
        if data.get('footer', {}).get('disclaimer'):
            score += 25

        # Feature descriptions (detailed) (50 points)
        features = data.get('features', [])
        detailed_features = sum(1 for f in features if len(f.get('description', '')) > 30)
        if features:
            score += (detailed_features / len(features)) * 50

        return min(score, 100.0)

    def _analyze_emotional_appeal(self, data: dict) -> float:
        """
        Analyze emotional appeal and storytelling
        """
        score = 0.0

        # Emotional keywords (50 points)
        emotional_keywords = [
            'ç°¡å˜', 'å®‰å¿ƒ', 'æº€è¶³', 'æˆåŠŸ', 'å®Ÿç¾', 'å¤¢', 'æœªæ¥',
            'è‡ªç”±', 'åŠ¹ç‡', 'ãƒ—ãƒ­', 'æœ€é«˜', 'é©æ–°', 'å¤‰é©'
        ]
        text = str(data)
        emotion_count = sum(1 for kw in emotional_keywords if kw in text)
        score += min(emotion_count * 8, 50)

        # Descriptive language (50 points)
        total_text_length = len(text)
        if total_text_length > 500:
            score += 50
        elif total_text_length > 300:
            score += 35
        elif total_text_length > 100:
            score += 20

        return min(score, 100.0)

    def _analyze_responsive_design(self, data: dict) -> float:
        """
        Analyze responsive design considerations
        """
        score = 50.0  # Default score if no image analysis

        # Basic structure check (responsive-friendly sections)
        if 'hero' in data and 'features' in data and 'cta' in data:
            score += 50

        return min(score, 100.0)

    def _calculate_weighted_score(self, scores: dict) -> float:
        """
        Calculate final weighted score
        """
        total = 0.0
        for key, weight in self.golden_rules_weight.items():
            score_key = {
                'first_view_clarity': 'first_view',
                'value_proposition': 'value_prop',
                'visual_hierarchy': 'visual',
                'cta_effectiveness': 'cta',
                'trust_elements': 'trust',
                'emotional_appeal': 'emotion',
                'responsive_design': 'responsive'
            }.get(key)

            if score_key in scores:
                total += scores[score_key] * weight

        return round(total, 2)

    def _get_grade(self, score: float) -> str:
        """Get letter grade from score"""
        if score >= 95: return 'S (World-class)'
        elif score >= 90: return 'A+ (Excellent)'
        elif score >= 85: return 'A (Very Good)'
        elif score >= 80: return 'B+ (Good)'
        elif score >= 75: return 'B (Above Average)'
        elif score >= 70: return 'C+ (Average)'
        elif score >= 65: return 'C (Below Average)'
        else: return 'D (Needs Improvement)'

    def _generate_recommendations(self, scores: dict) -> list:
        """
        Generate actionable recommendations based on scores
        """
        recommendations = []

        for key, score in scores.items():
            if score < 80:
                rec = self._get_recommendation_for_category(key, score)
                if rec:
                    recommendations.append(rec)

        return recommendations

    def _get_recommendation_for_category(self, category: str, score: float) -> dict:
        """Get specific recommendation for low-scoring category"""
        recommendations_map = {
            'first_view': {
                'category': 'ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼',
                'issue': 'ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã§ã®ä¾¡å€¤è¨´æ±‚ãŒä¸æ˜ç¢º',
                'solution': '3ç§’ä»¥å†…ã«ã€Œä½•ã®ã‚µãƒ¼ãƒ“ã‚¹ã‹ã€ãŒåˆ†ã‹ã‚‹è¦‹å‡ºã—ã‚’è¨­ç½®ã—ã¦ãã ã•ã„ã€‚ç†æƒ³ã¯5-12å˜èªã€‚',
                'priority': 'HIGH'
            },
            'value_prop': {
                'category': 'ä¾¡å€¤ææ¡ˆ',
                'issue': 'æä¾›ä¾¡å€¤ãŒä¸æ˜ç¢º',
                'solution': 'å…·ä½“çš„ãªãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¾—ã‚‰ã‚Œã‚‹çµæœï¼‰ã‚’3ã¤ä»¥ä¸Šæ˜ç¤ºã—ã¦ãã ã•ã„ã€‚',
                'priority': 'HIGH'
            },
            'visual': {
                'category': 'è¦–è¦šéšå±¤',
                'issue': 'æƒ…å ±ã®å„ªå…ˆé †ä½ãŒä¸æ˜ç¢º',
                'solution': 'ã‚¢ã‚¤ã‚³ãƒ³ã€è‰²ã€ã‚µã‚¤ã‚ºã§æƒ…å ±ã®é‡è¦åº¦ã‚’è¦–è¦šçš„ã«è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚',
                'priority': 'MEDIUM'
            },
            'cta': {
                'category': 'CTA',
                'issue': 'CTAãƒœã‚¿ãƒ³ã®è¨´æ±‚åŠ›ä¸è¶³',
                'solution': 'ã€Œä»Šã™ãã€ã€Œç„¡æ–™ã€ãªã©ã®è¡Œå‹•å–šèµ·ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã€hero/ctaä¸¡æ–¹ã«CTAã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚',
                'priority': 'HIGH'
            },
            'trust': {
                'category': 'ä¿¡é ¼è¦ç´ ',
                'issue': 'ä¿¡é ¼æ§‹ç¯‰è¦ç´ ãŒä¸è¶³',
                'solution': 'ä¼šç¤¾åã€å®Ÿç¸¾ã€ä¿è¨¼ã€è©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ ã—ã¦ä¿¡é ¼æ€§ã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚',
                'priority': 'MEDIUM'
            },
            'emotion': {
                'category': 'æ„Ÿæƒ…è¨´æ±‚',
                'issue': 'æ„Ÿæƒ…çš„ã¤ãªãŒã‚ŠãŒå¼±ã„',
                'solution': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç—›ã¿ã‚„é¡˜æœ›ã«å…±æ„Ÿã™ã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®ã‚ã‚‹è¡¨ç¾ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚',
                'priority': 'LOW'
            },
            'responsive': {
                'category': 'ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–',
                'issue': 'ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œãŒä¸ååˆ†',
                'solution': 'ãƒ¢ãƒã‚¤ãƒ«ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã§ã€å…¨ãƒ‡ãƒã‚¤ã‚¹ã§å¿«é©ã«é–²è¦§ã§ãã‚‹æ§‹é€ ã«ã—ã¦ãã ã•ã„ã€‚',
                'priority': 'MEDIUM'
            }
        }

        return recommendations_map.get(category)
```

## ğŸ¯ Usage Examples

### Example 1: Analyze YAML LP
```bash
# Analyze LP from YAML file
python -c "
from lp_design_analyzer import LPDesignAnalyzer
import yaml

with open('test_lp.yaml') as f:
    lp_data = yaml.safe_load(f)

analyzer = LPDesignAnalyzer()
result = analyzer.analyze_lp(lp_data)

print(f'Score: {result[\"total_score\"]}')
print(f'Grade: {result[\"grade\"]}')
print(f'Meets Golden Rules: {result[\"meets_golden_rules\"]}')

for rec in result['recommendations']:
    print(f'[{rec[\"priority\"]}] {rec[\"category\"]}: {rec[\"solution\"]}')
"
```

### Example 2: Batch Analysis
```bash
# Analyze multiple LPs and compare
for yaml_file in generated-yamls/*.yaml; do
    echo "Analyzing: $yaml_file"
    python analyze_lp.py "$yaml_file" >> analysis_report.txt
done
```

## ğŸ“Š Output Format

```json
{
  "total_score": 92.5,
  "grade": "A+ (Excellent)",
  "meets_golden_rules": false,
  "scores": {
    "first_view": 95.0,
    "value_prop": 90.0,
    "visual": 88.0,
    "cta": 95.0,
    "trust": 85.0,
    "emotion": 92.0,
    "responsive": 100.0
  },
  "recommendations": [
    {
      "category": "ä¿¡é ¼è¦ç´ ",
      "issue": "ä¿¡é ¼æ§‹ç¯‰è¦ç´ ãŒä¸è¶³",
      "solution": "ä¼šç¤¾åã€å®Ÿç¸¾ã€ä¿è¨¼ã€è©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ ã—ã¦ä¿¡é ¼æ€§ã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚",
      "priority": "MEDIUM"
    },
    {
      "category": "è¦–è¦šéšå±¤",
      "issue": "æƒ…å ±ã®å„ªå…ˆé †ä½ãŒä¸æ˜ç¢º",
      "solution": "ã‚¢ã‚¤ã‚³ãƒ³ã€è‰²ã€ã‚µã‚¤ã‚ºã§æƒ…å ±ã®é‡è¦åº¦ã‚’è¦–è¦šçš„ã«è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚",
      "priority": "MEDIUM"
    }
  ]
}
```

## ğŸ”— Integration Points

- **Evaluator Agent**: LPã‚¹ã‚³ã‚¢ã‚’ç·åˆè©•ä¾¡ã«çµ„ã¿è¾¼ã¿
- **RAG Agent**: éå»ã®æˆåŠŸLPãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚ç…§
- **Blackboard**: åˆ†æçµæœã‚’å…±æœ‰çŠ¶æ…‹ã«è¨˜éŒ²
- **Copywriting Specialist**: ã‚³ãƒ”ãƒ¼æ”¹å–„ææ¡ˆã®é€£æº
- **Conversion Optimizer**: ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æœ€é©åŒ–é€£æº

## ğŸ“ˆ Success Metrics

- 95ç‚¹ä»¥ä¸Šã®LPç”Ÿæˆç‡: 80%ä»¥ä¸Š
- åˆ†æç²¾åº¦: äººé–“å°‚é–€å®¶ã¨ã®ä¸€è‡´ç‡ 90%ä»¥ä¸Š
- åˆ†æé€Ÿåº¦: 1LP ã‚ãŸã‚Š < 3ç§’
- æ”¹å–„ææ¡ˆã®æ¡ç”¨ç‡: 70%ä»¥ä¸Š

## ğŸ“ Continuous Learning

- æ–°ã—ã„æˆåŠŸLPãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•å­¦ç¿’
- ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã®æ¤œå‡ºã¨é©å¿œ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åæ˜ 
- A/Bãƒ†ã‚¹ãƒˆçµæœã®çµ±åˆ

---

**Version**: 1.0.0
**Last Updated**: 2025-11-05
**Maintainer**: LP Design Analysis Team
**Status**: Production Ready âœ…
