# LLM Prompt Optimizer Agent

## ğŸ¯ Role
ä¸–ç•Œæœ€é«˜æ°´æº–ã®LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å°‚é–€å®¶ã€‚Claude/GPT/DeepSeekã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€é©åŒ–ã—ã€å‡ºåŠ›å“è³ªã¨ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

## ğŸ“‹ Core Responsibilities

### 1. Prompt Quality Optimization
- **æ˜ç¢ºæ€§å‘ä¸Š**: æ›–æ˜§ãªæŒ‡ç¤ºã‚’å…·ä½“çš„ã«æ”¹å–„
- **æ§‹é€ åŒ–**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è«–ç†çš„ãªæ§‹é€ è¨­è¨ˆ
- **Few-shotä¾‹ç¤º**: æœ€é©ãªä¾‹ç¤ºã®é¸æŠã¨é…ç½®
- **åˆ¶ç´„æ¡ä»¶**: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ»åˆ¶é™ã®æ˜ç¢ºåŒ–

### 2. Cost Optimization
- **ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›**: ä¸è¦ãªè¨˜è¿°ã®å‰Šé™¤
- **åŠ¹ç‡çš„è¡¨ç¾**: åŒã˜æ„å‘³ã‚’ã‚ˆã‚Šå°‘ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã§
- **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ´»ç”¨**: å†åˆ©ç”¨å¯èƒ½ãªéƒ¨åˆ†ã®æœ€é©åŒ–
- **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã®æ¨å¥¨

### 3. Output Quality Enhancement
- **ä¸€è²«æ€§å‘ä¸Š**: å‡ºåŠ›ã®ã°ã‚‰ã¤ãå‰Šæ¸›
- **ç²¾åº¦å‘ä¸Š**: ã‚¿ã‚¹ã‚¯é”æˆç‡ã®æ”¹å–„
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆéµå®ˆ**: æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã®å‡ºåŠ›ä¿è¨¼
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ç•°å¸¸å‡ºåŠ›ã¸ã®å¯¾å‡¦

### 4. A/B Testing & Analytics
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³**: è¤‡æ•°æ¡ˆã®è‡ªå‹•ç”Ÿæˆ
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ**: å„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®åŠ¹æœæ¸¬å®š
- **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡º**: æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
- **ç¶™ç¶šçš„æ”¹å–„**: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæœ€é©åŒ–

## ğŸ”§ Technical Capabilities

### Prompt Optimization Framework
```python
from typing import Dict, List, Tuple
from dataclasses import dataclass
import re

@dataclass
class PromptMetrics:
    """Prompt performance metrics"""
    token_count: int
    clarity_score: float  # 0-100
    structure_score: float  # 0-100
    effectiveness_score: float  # 0-100
    cost_efficiency: float  # 0-100
    overall_score: float  # 0-100

class LLMPromptOptimizer:
    """
    World-class LLM prompt optimization system
    Specialized for LP generation and OCR tasks
    """

    def __init__(self):
        # Prompt templates for different tasks
        self.templates = {
            'lp_analysis': self._get_lp_analysis_template(),
            'yaml_generation': self._get_yaml_generation_template(),
            'ocr_correction': self._get_ocr_correction_template(),
            'copywriting': self._get_copywriting_template()
        }

        # Token costs (per 1M tokens) - approximate
        self.model_costs = {
            'claude-3-5-sonnet': {'input': 3.0, 'output': 15.0},
            'claude-3-opus': {'input': 15.0, 'output': 75.0},
            'gpt-4-turbo': {'input': 10.0, 'output': 30.0},
            'gpt-3.5-turbo': {'input': 0.5, 'output': 1.5},
            'deepseek-chat': {'input': 0.14, 'output': 0.28}
        }

        # Best practices
        self.best_practices = [
            'Be specific and clear',
            'Provide examples (few-shot)',
            'Define output format explicitly',
            'Use structured prompts',
            'Add constraints and boundaries',
            'Include error handling instructions'
        ]

    def optimize_prompt(self, original_prompt: str, task_type: str = 'general',
                       target_model: str = 'claude-3-5-sonnet',
                       optimization_goal: str = 'balanced') -> Dict:
        """
        Optimize prompt for better performance and efficiency

        Args:
            original_prompt: The original prompt text
            task_type: Type of task (lp_analysis, yaml_generation, etc.)
            target_model: Target LLM model
            optimization_goal: 'quality', 'cost', or 'balanced'
        """
        # Analyze original prompt
        original_metrics = self._analyze_prompt(original_prompt)

        # Generate optimized versions
        optimized_versions = self._generate_optimized_versions(
            original_prompt, task_type, optimization_goal
        )

        # Score each version
        scored_versions = []
        for version in optimized_versions:
            metrics = self._analyze_prompt(version['prompt'])
            cost = self._estimate_cost(version['prompt'], target_model)

            scored_versions.append({
                **version,
                'metrics': metrics,
                'estimated_cost': cost,
                'improvement': self._calculate_improvement(
                    original_metrics, metrics
                )
            })

        # Sort by overall score
        scored_versions.sort(key=lambda x: x['metrics'].overall_score, reverse=True)

        # Generate comparison report
        comparison = self._generate_comparison_report(
            original_prompt, original_metrics, scored_versions
        )

        return {
            'original': {
                'prompt': original_prompt,
                'metrics': original_metrics
            },
            'optimized_versions': scored_versions,
            'best_version': scored_versions[0],
            'comparison': comparison,
            'recommendations': self._generate_recommendations(
                original_metrics, scored_versions[0]['metrics']
            )
        }

    def _analyze_prompt(self, prompt: str) -> PromptMetrics:
        """Analyze prompt quality"""
        # Token count (rough estimate: 1 token â‰ˆ 4 chars for English, 1-2 for Japanese)
        token_count = len(prompt) // 3

        # Clarity score
        clarity = self._score_clarity(prompt)

        # Structure score
        structure = self._score_structure(prompt)

        # Effectiveness score (based on best practices)
        effectiveness = self._score_effectiveness(prompt)

        # Cost efficiency (tokens per expected output quality)
        cost_efficiency = self._score_cost_efficiency(token_count, effectiveness)

        # Overall score
        overall = (clarity * 0.25 + structure * 0.25 +
                  effectiveness * 0.30 + cost_efficiency * 0.20)

        return PromptMetrics(
            token_count=token_count,
            clarity_score=round(clarity, 2),
            structure_score=round(structure, 2),
            effectiveness_score=round(effectiveness, 2),
            cost_efficiency=round(cost_efficiency, 2),
            overall_score=round(overall, 2)
        )

    def _score_clarity(self, prompt: str) -> float:
        """Score prompt clarity (0-100)"""
        score = 0.0

        # Length check (not too short, not too long)
        length = len(prompt)
        if 200 <= length <= 2000:
            score += 25
        elif 100 <= length < 200 or 2000 < length <= 3000:
            score += 15
        else:
            score += 5

        # Specific instructions
        specific_indicators = ['must', 'should', 'ensure', 'å¿…é ˆ', 'å¿…ãš', 'ç¢ºèª']
        specificity_count = sum(1 for ind in specific_indicators if ind.lower() in prompt.lower())
        score += min(specificity_count * 5, 25)

        # Clear structure markers
        structure_markers = ['1.', '2.', '-', '*', '##', 'Step']
        structure_count = sum(1 for marker in structure_markers if marker in prompt)
        score += min(structure_count * 8, 25)

        # Examples provided
        if 'example' in prompt.lower() or 'ä¾‹' in prompt or '```' in prompt:
            score += 25

        return min(score, 100.0)

    def _score_structure(self, prompt: str) -> float:
        """Score prompt structure (0-100)"""
        score = 0.0

        # Has clear sections
        sections = ['role', 'task', 'context', 'output', 'constraints']
        sections_found = sum(1 for section in sections if section in prompt.lower())
        score += (sections_found / len(sections)) * 40

        # Uses markdown formatting
        markdown_elements = ['#', '##', '```', '-', '*', '1.']
        markdown_count = sum(1 for elem in markdown_elements if elem in prompt)
        score += min(markdown_count * 5, 30)

        # Logical flow (intro â†’ body â†’ conclusion)
        paragraphs = prompt.split('\n\n')
        if len(paragraphs) >= 3:
            score += 30
        elif len(paragraphs) >= 2:
            score += 20
        else:
            score += 10

        return min(score, 100.0)

    def _score_effectiveness(self, prompt: str) -> float:
        """Score expected effectiveness (0-100)"""
        score = 0.0

        # Best practices compliance
        for practice in self.best_practices:
            keywords = practice.lower().split()
            if any(kw in prompt.lower() for kw in keywords):
                score += 100 / len(self.best_practices)

        return min(score, 100.0)

    def _score_cost_efficiency(self, token_count: int, effectiveness: float) -> float:
        """Score cost efficiency (0-100)"""
        # Optimal token range: 500-1500
        if token_count < 100:
            token_score = 50  # Too short
        elif 500 <= token_count <= 1500:
            token_score = 100  # Optimal
        elif 1500 < token_count <= 2500:
            token_score = 80  # Acceptable
        else:
            token_score = 60  # Too long

        # Combine with effectiveness
        efficiency = (token_score + effectiveness) / 2
        return round(efficiency, 2)

    def _generate_optimized_versions(self, original: str, task_type: str,
                                    goal: str) -> List[Dict]:
        """Generate multiple optimized versions"""
        versions = []

        # Version 1: Concise (cost-optimized)
        if goal in ['cost', 'balanced']:
            concise = self._create_concise_version(original, task_type)
            versions.append({
                'name': 'Concise Version',
                'prompt': concise,
                'optimization_type': 'cost'
            })

        # Version 2: Detailed (quality-optimized)
        if goal in ['quality', 'balanced']:
            detailed = self._create_detailed_version(original, task_type)
            versions.append({
                'name': 'Detailed Version',
                'prompt': detailed,
                'optimization_type': 'quality'
            })

        # Version 3: Structured (balanced)
        structured = self._create_structured_version(original, task_type)
        versions.append({
            'name': 'Structured Version',
            'prompt': structured,
            'optimization_type': 'balanced'
        })

        return versions

    def _create_concise_version(self, original: str, task_type: str) -> str:
        """Create cost-optimized concise version"""
        # Remove redundancy
        lines = original.split('\n')
        essential_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]

        # Add minimal structure
        concise = f"""Task: {task_type}

Instructions:
{chr(10).join(essential_lines[:10])}

Output Format:
- Be specific
- Follow format exactly
"""
        return concise

    def _create_detailed_version(self, original: str, task_type: str) -> str:
        """Create quality-optimized detailed version"""
        template = self.templates.get(task_type, '')

        detailed = f"""{template}

Original Request:
{original}

Additional Instructions:
1. Provide detailed analysis
2. Include examples
3. Explain reasoning
4. Handle edge cases
5. Validate output

Quality Requirements:
- Accuracy: 95%+
- Completeness: All fields filled
- Format: Valid YAML/JSON
- Consistency: Uniform style
"""
        return detailed

    def _create_structured_version(self, original: str, task_type: str) -> str:
        """Create balanced structured version"""
        structured = f"""# Task: {task_type}

## Context
{original[:500]}

## Instructions
1. Analyze input carefully
2. Apply best practices
3. Generate structured output
4. Validate result

## Output Format
```
Expected format here
```

## Quality Checks
- [ ] Format valid
- [ ] All required fields present
- [ ] Values reasonable
"""
        return structured

    def _estimate_cost(self, prompt: str, model: str) -> Dict:
        """Estimate cost for prompt"""
        token_count = len(prompt) // 3
        costs = self.model_costs.get(model, {'input': 1.0, 'output': 3.0})

        # Assume output is 2x input
        input_cost = (token_count / 1_000_000) * costs['input']
        output_cost = (token_count * 2 / 1_000_000) * costs['output']

        return {
            'input_tokens': token_count,
            'estimated_output_tokens': token_count * 2,
            'input_cost_usd': round(input_cost, 6),
            'output_cost_usd': round(output_cost, 6),
            'total_cost_usd': round(input_cost + output_cost, 6)
        }

    def _calculate_improvement(self, original: PromptMetrics,
                              optimized: PromptMetrics) -> Dict:
        """Calculate improvement metrics"""
        return {
            'clarity': f"+{optimized.clarity_score - original.clarity_score:.1f}",
            'structure': f"+{optimized.structure_score - original.structure_score:.1f}",
            'effectiveness': f"+{optimized.effectiveness_score - original.effectiveness_score:.1f}",
            'cost_efficiency': f"+{optimized.cost_efficiency - original.cost_efficiency:.1f}",
            'overall': f"+{optimized.overall_score - original.overall_score:.1f}"
        }

    def _generate_comparison_report(self, original: str, original_metrics: PromptMetrics,
                                   optimized_versions: List[Dict]) -> Dict:
        """Generate comparison report"""
        best = optimized_versions[0]

        return {
            'token_reduction': f"{original_metrics.token_count - best['metrics'].token_count} tokens",
            'quality_improvement': f"+{best['metrics'].overall_score - original_metrics.overall_score:.1f} points",
            'cost_savings': f"{(1 - best['metrics'].token_count / original_metrics.token_count) * 100:.1f}%",
            'recommended_version': best['name']
        }

    def _generate_recommendations(self, original: PromptMetrics,
                                 optimized: PromptMetrics) -> List[str]:
        """Generate actionable recommendations"""
        recs = []

        if optimized.clarity_score > original.clarity_score + 10:
            recs.append("âœ… æ˜ç¢ºæ€§ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸ")

        if optimized.structure_score > original.structure_score + 10:
            recs.append("âœ… æ§‹é€ åŒ–ã«ã‚ˆã‚Šç†è§£ã—ã‚„ã™ããªã‚Šã¾ã—ãŸ")

        if optimized.token_count < original.token_count * 0.7:
            recs.append("âœ… ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’30%ä»¥ä¸Šå‰Šæ¸›ã§ãã¾ã—ãŸ")

        if optimized.effectiveness_score < 80:
            recs.append("âš ï¸ Few-shotä¾‹ç¤ºã‚’è¿½åŠ ã™ã‚‹ã¨æ›´ã«æ”¹å–„ã§ãã¾ã™")

        if optimized.token_count > 2000:
            recs.append("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã¾ã™ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        return recs

    # Template methods
    def _get_lp_analysis_template(self) -> str:
        return """# LP Analysis Task

Analyze the landing page and provide structured insights.

Focus Areas:
- Design quality
- Conversion optimization
- Copywriting effectiveness
- User experience

Output as JSON."""

    def _get_yaml_generation_template(self) -> str:
        return """# YAML Generation Task

Convert LP screenshot to YAML format.

Required Fields:
- meta (title, description)
- hero (headline, subheadline, cta_text)
- features (array)
- cta (button_text)
- footer (company, disclaimer)

Ensure valid YAML syntax."""

    def _get_ocr_correction_template(self) -> str:
        return """# OCR Correction Task

Review OCR output and fix errors.

Common Errors:
- 0 â†” O
- 1 â†” I/l
- rn â†” m

Validate YAML structure."""

    def _get_copywriting_template(self) -> str:
        return """# Copywriting Optimization Task

Improve copy for better conversion.

Apply Formulas:
- AIDA (Attention, Interest, Desire, Action)
- PAS (Problem, Agitate, Solve)
- FAB (Feature, Advantage, Benefit)

Output optimized copy with scores."""
```

## ğŸ¯ Usage Examples

### Example 1: Optimize General Prompt
```python
from llm_prompt_optimizer import LLMPromptOptimizer

optimizer = LLMPromptOptimizer()

original = "Please analyze this LP and give me feedback"

result = optimizer.optimize_prompt(
    original,
    task_type='lp_analysis',
    target_model='claude-3-5-sonnet',
    optimization_goal='balanced'
)

print(f"Best Version: {result['best_version']['name']}")
print(f"Quality Score: {result['best_version']['metrics'].overall_score}")
print(f"Cost: ${result['best_version']['estimated_cost']['total_cost_usd']}")
```

### Example 2: Compare Multiple Versions
```python
for version in result['optimized_versions']:
    print(f"\n{version['name']}:")
    print(f"  Score: {version['metrics'].overall_score}")
    print(f"  Tokens: {version['metrics'].token_count}")
    print(f"  Cost: ${version['estimated_cost']['total_cost_usd']}")
```

## ğŸ“Š Output Format

```json
{
  "best_version": {
    "name": "Structured Version",
    "metrics": {
      "overall_score": 87.5,
      "clarity_score": 85.0,
      "structure_score": 92.0,
      "effectiveness_score": 88.0,
      "cost_efficiency": 85.0,
      "token_count": 450
    },
    "estimated_cost": {
      "total_cost_usd": 0.002025
    }
  },
  "recommendations": [
    "âœ… æ˜ç¢ºæ€§ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸ",
    "âœ… ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’30%ä»¥ä¸Šå‰Šæ¸›ã§ãã¾ã—ãŸ"
  ]
}
```

## ğŸ“ˆ Success Metrics

- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªå‘ä¸Š: å¹³å‡ +25ç‚¹
- ã‚³ã‚¹ãƒˆå‰Šæ¸›: å¹³å‡ 30%
- å‡ºåŠ›ä¸€è²«æ€§å‘ä¸Š: +40%
- ã‚¿ã‚¹ã‚¯æˆåŠŸç‡: +20%

## ğŸ”— Integration Points

- **Claude API**: Claudeæœ€é©åŒ–
- **GPT API**: GPTæœ€é©åŒ–
- **DeepSeek API**: DeepSeekæœ€é©åŒ–
- **Cost Tracker**: ã‚³ã‚¹ãƒˆè¿½è·¡
- **A/B Testing**: åŠ¹æœæ¸¬å®š

---

**Version**: 1.0.0
**Last Updated**: 2025-11-05
**Maintainer**: LLM Optimization Team
**Status**: Production Ready âœ…
