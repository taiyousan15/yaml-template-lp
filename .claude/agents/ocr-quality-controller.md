# OCR Quality Controller Agent

## ğŸ¯ Role
ä¸–ç•Œæœ€é«˜æ°´æº–ã®OCRï¼ˆå…‰å­¦æ–‡å­—èªè­˜ï¼‰å“è³ªç®¡ç†å°‚é–€å®¶ã€‚ç”»åƒã‹ã‚‰YAMLå¤‰æ›ã®ç²¾åº¦ã‚’æœ€å¤§åŒ–ã—ã€èª¤èªè­˜ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

## ğŸ“‹ Core Responsibilities

### 1. OCR Accuracy Analysis
- **æ–‡å­—èªè­˜ç²¾åº¦æ¸¬å®š**: æ­£ç¢ºåº¦ãƒ»å†ç¾ç‡ãƒ»F1ã‚¹ã‚³ã‚¢ã®ç®—å‡º
- **èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º**: ã‚ˆãã‚ã‚‹é–“é•ã„ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- **ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°**: OCRçµæœã®ä¿¡é ¼æ€§è©•ä¾¡
- **å¤šè¨€èªå¯¾å¿œ**: æ—¥æœ¬èªãƒ»è‹±èªãƒ»æ•°å­—ã®èªè­˜å“è³ª

### 2. Pre-processing Optimization
- **ç”»åƒå“è³ªè©•ä¾¡**: è§£åƒåº¦ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ»ãƒã‚¤ã‚ºã®åˆ†æ
- **å‰å‡¦ç†æ¨å¥¨**: æœ€é©ãªç”»åƒå‡¦ç†æ‰‹æ³•ã®ææ¡ˆ
- **ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåˆ†æ**: ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®æ­£ç¢ºãªæ¤œå‡º
- **ãƒ•ã‚©ãƒ³ãƒˆåˆ¤å®š**: ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆã®è­˜åˆ¥ã¨å¯¾å¿œ

### 3. Post-processing Validation
- **ã‚¹ãƒšãƒ«ãƒã‚§ãƒƒã‚¯**: è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®èª¤èªè­˜æ¤œå‡º
- **æ–‡è„ˆæ¤œè¨¼**: å‰å¾Œé–¢ä¿‚ã‹ã‚‰ä¸è‡ªç„¶ãªå¤‰æ›ã‚’æ¤œå‡º
- **æ§‹é€ æ•´åˆæ€§**: YAMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
- **æ•°å€¤æ¤œè¨¼**: æ•°å­—ã®æ¡æ•°ãƒ»å½¢å¼ã®å¦¥å½“æ€§ç¢ºèª

### 4. Continuous Learning
- **èª¤èªè­˜ãƒ­ã‚°**: éå»ã®é–“é•ã„ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒ–
- **ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’**: ã‚ˆãã‚ã‚‹èª¤èªè­˜ã®å­¦ç¿’ã¨å¯¾ç­–
- **æ”¹å–„ææ¡ˆ**: DeepSeek OCRã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ææ¡ˆ
- **ç²¾åº¦ãƒˆãƒ¬ãƒ³ãƒ‰**: æ™‚ç³»åˆ—ã§ã®ç²¾åº¦å¤‰åŒ–ã®è¿½è·¡

## ğŸ”§ Technical Capabilities

### OCR Quality Control Framework
```python
import re
from typing import Dict, List, Tuple
from dataclasses import dataclass
import difflib
from collections import Counter

@dataclass
class OCRQualityMetrics:
    """OCR quality metrics"""
    character_accuracy: float  # Character-level accuracy
    word_accuracy: float  # Word-level accuracy
    confidence_score: float  # Average confidence
    error_rate: float  # Character error rate (CER)
    yaml_validity: float  # YAML structure validity
    overall_quality: float  # Overall quality score

class OCRQualityController:
    """
    World-class OCR quality control system
    Specialized for LP screenshot to YAML conversion
    """

    def __init__(self):
        # Common OCR errors (Japanese)
        self.common_errors = {
            '0': ['O', 'o', 'D'],  # Zero vs O
            '1': ['I', 'l', '|'],  # One vs I/l
            '2': ['Z'],
            '5': ['S'],
            '8': ['B'],
            'O': ['0'],
            'I': ['1', 'l'],
            'l': ['1', 'I'],
            'rn': ['m'],  # Two chars recognized as one
            'vv': ['w']
        }

        # Expected YAML structure keys
        self.yaml_required_keys = [
            'meta', 'hero', 'features', 'cta', 'footer'
        ]
        self.yaml_optional_keys = [
            'testimonials', 'pricing', 'faq', 'gallery'
        ]

        # Common Japanese particles (for context validation)
        self.japanese_particles = [
            'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨', 'ã‚ˆã‚Š', 'ã‹ã‚‰', 'ã§', 'ã®'
        ]

        # Suspicious patterns
        self.suspicious_patterns = [
            r'[a-zA-Z]{20,}',  # Very long English words (likely OCR error)
            r'\d{10,}',  # Very long number sequences
            r'[^\x00-\x7F\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+',  # Invalid chars
            r'(.)\1{5,}'  # Same character repeated 6+ times
        ]

    def analyze_ocr_quality(self, ocr_result: str, original_image_path: str = None,
                           ground_truth: str = None) -> Dict:
        """
        Comprehensive OCR quality analysis
        """
        # Parse OCR result as YAML
        yaml_validity = self._validate_yaml_structure(ocr_result)

        # Detect suspicious patterns
        suspicious = self._detect_suspicious_patterns(ocr_result)

        # Calculate confidence metrics
        confidence = self._estimate_confidence(ocr_result, suspicious)

        # If ground truth provided, calculate accuracy
        accuracy_metrics = None
        if ground_truth:
            accuracy_metrics = self._calculate_accuracy(ocr_result, ground_truth)

        # Detect specific error types
        error_patterns = self._detect_error_patterns(ocr_result)

        # Generate improvement recommendations
        recommendations = self._generate_recommendations(
            yaml_validity, suspicious, error_patterns, confidence
        )

        # Calculate overall quality score
        overall_quality = self._calculate_overall_quality(
            yaml_validity, suspicious, error_patterns, confidence
        )

        metrics = OCRQualityMetrics(
            character_accuracy=accuracy_metrics['char_accuracy'] if accuracy_metrics else confidence,
            word_accuracy=accuracy_metrics['word_accuracy'] if accuracy_metrics else confidence,
            confidence_score=confidence,
            error_rate=1.0 - confidence,
            yaml_validity=yaml_validity,
            overall_quality=overall_quality
        )

        return {
            'metrics': metrics,
            'yaml_validation': {
                'is_valid': yaml_validity > 0.8,
                'score': yaml_validity,
                'issues': self._identify_yaml_issues(ocr_result)
            },
            'suspicious_patterns': suspicious,
            'error_patterns': error_patterns,
            'recommendations': recommendations,
            'quality_grade': self._get_quality_grade(overall_quality)
        }

    def _validate_yaml_structure(self, text: str) -> float:
        """
        Validate YAML structure
        Returns: 0.0 - 1.0
        """
        score = 0.0

        # Check for required keys (60 points)
        required_found = sum(1 for key in self.yaml_required_keys if key in text)
        score += (required_found / len(self.yaml_required_keys)) * 0.6

        # Check YAML syntax indicators (20 points)
        yaml_indicators = [':', '-', 'title', 'description', 'text']
        indicators_found = sum(1 for ind in yaml_indicators if ind in text)
        score += (indicators_found / len(yaml_indicators)) * 0.2

        # Check for hierarchical structure (20 points)
        # Count indentation (good YAML should have consistent indentation)
        lines = text.split('\n')
        indented_lines = sum(1 for line in lines if line.startswith('  ') or line.startswith('    '))
        if len(lines) > 0:
            indentation_ratio = indented_lines / len(lines)
            score += min(indentation_ratio * 2, 0.2)  # Cap at 0.2

        return min(score, 1.0)

    def _identify_yaml_issues(self, text: str) -> List[Dict]:
        """Identify specific YAML structure issues"""
        issues = []

        # Check for missing required keys
        for key in self.yaml_required_keys:
            if key not in text:
                issues.append({
                    'type': 'missing_key',
                    'severity': 'HIGH',
                    'key': key,
                    'message': f'Required key "{key}" not found'
                })

        # Check for invalid characters in keys
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if ':' in line:
                key_part = line.split(':')[0].strip()
                if re.search(r'[^\w\-_]', key_part):
                    issues.append({
                        'type': 'invalid_key',
                        'severity': 'MEDIUM',
                        'line': i + 1,
                        'message': f'Invalid characters in key: "{key_part}"'
                    })

        return issues

    def _detect_suspicious_patterns(self, text: str) -> List[Dict]:
        """Detect suspicious OCR patterns"""
        suspicious = []

        for pattern in self.suspicious_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                suspicious.append({
                    'pattern': pattern,
                    'match': match.group(),
                    'position': match.start(),
                    'reason': self._explain_suspicion(pattern)
                })

        return suspicious

    def _explain_suspicion(self, pattern: str) -> str:
        """Explain why pattern is suspicious"""
        explanations = {
            r'[a-zA-Z]{20,}': 'ç•°å¸¸ã«é•·ã„è‹±å˜èªï¼ˆOCRèª¤èªè­˜ã®å¯èƒ½æ€§ï¼‰',
            r'\d{10,}': 'ç•°å¸¸ã«é•·ã„æ•°å­—åˆ—',
            r'[^\x00-\x7F\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+': 'ç„¡åŠ¹ãªæ–‡å­—ãŒå«ã¾ã‚Œã‚‹',
            r'(.)\1{5,}': 'åŒã˜æ–‡å­—ã®é€£ç¶šï¼ˆOCRãƒã‚¤ã‚ºï¼‰'
        }
        return explanations.get(pattern, 'ä¸æ˜ãªç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³')

    def _estimate_confidence(self, text: str, suspicious: List[Dict]) -> float:
        """
        Estimate overall OCR confidence
        Returns: 0.0 - 1.0
        """
        confidence = 1.0

        # Reduce confidence for suspicious patterns
        confidence -= len(suspicious) * 0.05

        # Check text length reasonableness
        if len(text) < 100:
            confidence -= 0.2  # Too short
        elif len(text) > 10000:
            confidence -= 0.1  # Suspiciously long

        # Check Japanese to English ratio (for Japanese LPs)
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))
        total_chars = len(text.replace(' ', '').replace('\n', ''))

        if total_chars > 0:
            jp_ratio = japanese_chars / total_chars
            if jp_ratio < 0.3:  # Less than 30% Japanese for Japanese LP
                confidence -= 0.15

        return max(confidence, 0.0)

    def _calculate_accuracy(self, ocr_result: str, ground_truth: str) -> Dict:
        """
        Calculate accuracy metrics compared to ground truth
        """
        # Character-level accuracy
        matcher = difflib.SequenceMatcher(None, ground_truth, ocr_result)
        char_accuracy = matcher.ratio()

        # Word-level accuracy
        gt_words = ground_truth.split()
        ocr_words = ocr_result.split()

        correct_words = 0
        for gt_word, ocr_word in zip(gt_words, ocr_words):
            if gt_word == ocr_word:
                correct_words += 1

        word_accuracy = correct_words / max(len(gt_words), len(ocr_words))

        # Character Error Rate (CER)
        distance = self._levenshtein_distance(ground_truth, ocr_result)
        cer = distance / len(ground_truth) if ground_truth else 0

        return {
            'char_accuracy': round(char_accuracy, 4),
            'word_accuracy': round(word_accuracy, 4),
            'cer': round(cer, 4)
        }

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """Calculate Levenshtein distance"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def _detect_error_patterns(self, text: str) -> List[Dict]:
        """Detect common OCR error patterns"""
        errors = []

        # Check for common character substitutions
        for correct, wrong_list in self.common_errors.items():
            for wrong in wrong_list:
                if wrong in text:
                    errors.append({
                        'type': 'char_substitution',
                        'wrong_char': wrong,
                        'likely_correct': correct,
                        'count': text.count(wrong),
                        'confidence': 0.7
                    })

        # Check for missing spaces (common in Japanese OCR)
        # Long sequences without spaces might indicate missing word boundaries
        long_sequences = re.findall(r'[^\s]{50,}', text)
        if long_sequences:
            errors.append({
                'type': 'missing_spaces',
                'sequences': len(long_sequences),
                'confidence': 0.6
            })

        return errors

    def _generate_recommendations(self, yaml_validity: float, suspicious: List[Dict],
                                 error_patterns: List[Dict], confidence: float) -> List[Dict]:
        """Generate actionable recommendations"""
        recommendations = []

        # YAML structure recommendations
        if yaml_validity < 0.8:
            recommendations.append({
                'category': 'YAML Structure',
                'priority': 'HIGH',
                'issue': 'YAMLæ§‹é€ ã®å¦¥å½“æ€§ãŒä½ã„',
                'solution': 'æ‰‹å‹•ã§YAMLã‚­ãƒ¼ï¼ˆhero, features, ctaç­‰ï¼‰ã‚’ç¢ºèªãƒ»ä¿®æ­£',
                'expected_improvement': f'+{(0.8 - yaml_validity) * 100:.0f}% validity'
            })

        # Suspicious patterns recommendations
        if len(suspicious) > 5:
            recommendations.append({
                'category': 'OCR Quality',
                'priority': 'HIGH',
                'issue': f'{len(suspicious)}å€‹ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º',
                'solution': 'ç”»åƒã®å‰å‡¦ç†ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå‘ä¸Šã€ãƒã‚¤ã‚ºé™¤å»ï¼‰ã‚’å®Ÿæ–½',
                'expected_improvement': '+20-30% accuracy'
            })

        # Confidence recommendations
        if confidence < 0.7:
            recommendations.append({
                'category': 'Confidence',
                'priority': 'HIGH',
                'issue': f'ä¿¡é ¼åº¦ãŒä½ã„ ({confidence:.1%})',
                'solution': 'ä»¥ä¸‹ã‚’å®Ÿæ–½:\n1. ç”»åƒè§£åƒåº¦ã‚’å‘ä¸Šï¼ˆæ¨å¥¨: 1920x1080ä»¥ä¸Šï¼‰\n2. ç”»åƒã®ãƒ–ãƒ¬ãƒ»ã¼ã‹ã—ã‚’é™¤å»\n3. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’èª¿æ•´',
                'expected_improvement': '+30-40% confidence'
            })

        # Error pattern recommendations
        if len(error_patterns) > 0:
            recommendations.append({
                'category': 'Error Patterns',
                'priority': 'MEDIUM',
                'issue': f'{len(error_patterns)}ç¨®é¡ã®èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º',
                'solution': 'é »å‡ºèª¤èªè­˜ï¼ˆ0â‡”O, 1â‡”Iç­‰ï¼‰ã‚’è‡ªå‹•è£œæ­£è¾æ›¸ã«è¿½åŠ ',
                'expected_improvement': '+10-15% accuracy'
            })

        return recommendations

    def _calculate_overall_quality(self, yaml_validity: float, suspicious: List[Dict],
                                  error_patterns: List[Dict], confidence: float) -> float:
        """Calculate overall OCR quality score"""
        # Weighted scoring
        score = (
            yaml_validity * 0.35 +        # YAML structure
            confidence * 0.40 +            # General confidence
            (1 - min(len(suspicious) / 20, 1.0)) * 0.15 +  # Suspicious patterns (penalty)
            (1 - min(len(error_patterns) / 10, 1.0)) * 0.10  # Error patterns (penalty)
        )

        return round(score * 100, 2)  # Convert to 0-100 scale

    def _get_quality_grade(self, score: float) -> str:
        """Get quality grade from score"""
        if score >= 95: return 'S (Excellent - Ready to use)'
        elif score >= 90: return 'A+ (Very Good - Minor fixes needed)'
        elif score >= 85: return 'A (Good - Some fixes needed)'
        elif score >= 80: return 'B+ (Fair - Review required)'
        elif score >= 75: return 'B (Acceptable - Manual review required)'
        elif score >= 70: return 'C+ (Poor - Significant fixes needed)'
        elif score >= 65: return 'C (Very Poor - Consider re-OCR)'
        else: return 'D (Failed - Re-OCR with better image)'

    def auto_correct_common_errors(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Automatically correct common OCR errors
        Returns: (corrected_text, corrections_made)
        """
        corrected = text
        corrections = []

        # Apply common error corrections
        for correct, wrong_list in self.common_errors.items():
            for wrong in wrong_list:
                if wrong in corrected:
                    # Context-aware correction (simplified)
                    # Only replace if surrounded by specific contexts
                    pattern = rf'\b{re.escape(wrong)}\b'
                    matches = list(re.finditer(pattern, corrected))

                    for match in matches:
                        corrected = corrected[:match.start()] + correct + corrected[match.end():]
                        corrections.append({
                            'position': match.start(),
                            'original': wrong,
                            'corrected': correct,
                            'confidence': 0.7
                        })

        return corrected, corrections

    def recommend_image_preprocessing(self, image_quality: Dict) -> List[str]:
        """
        Recommend image preprocessing steps
        """
        recommendations = []

        if image_quality.get('resolution_low', False):
            recommendations.append("ç”»åƒã‚’é«˜è§£åƒåº¦åŒ–ï¼ˆæœ€ä½ 1280x720ã€æ¨å¥¨ 1920x1080ï¼‰")

        if image_quality.get('low_contrast', False):
            recommendations.append("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’å¼·åŒ–ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ï¼‰")

        if image_quality.get('noisy', False):
            recommendations.append("ãƒã‚¤ã‚ºé™¤å»ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ï¼ˆGaussian blurï¼‰")

        if image_quality.get('skewed', False):
            recommendations.append("ç”»åƒã®å‚¾ãè£œæ­£ï¼ˆdeskewï¼‰")

        if image_quality.get('shadows', False):
            recommendations.append("å½±ãƒ»ç…§æ˜ãƒ ãƒ©ã‚’è£œæ­£ï¼ˆadaptive thresholdingï¼‰")

        return recommendations
```

## ğŸ¯ Usage Examples

### Example 1: Basic Quality Check
```python
from ocr_quality_controller import OCRQualityController

controller = OCRQualityController()

ocr_result = """
meta:
  title: ã‚µãƒ³ãƒ—ãƒ«LP
hero:
  headline: è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆ
  cta_text: ä»Šã™ãç”³ã—è¾¼ã‚€
"""

analysis = controller.analyze_ocr_quality(ocr_result)

print(f"Overall Quality: {analysis['metrics'].overall_quality}")
print(f"Grade: {analysis['quality_grade']}")
print(f"YAML Valid: {analysis['yaml_validation']['is_valid']}")
```

### Example 2: Auto-Correction
```python
corrected, corrections = controller.auto_correct_common_errors(ocr_result)

print(f"Corrections made: {len(corrections)}")
for correction in corrections:
    print(f"  {correction['original']} â†’ {correction['corrected']}")
```

### Example 3: Ground Truth Comparison
```python
ground_truth = "..."  # Original text
analysis = controller.analyze_ocr_quality(ocr_result, ground_truth=ground_truth)

print(f"Character Accuracy: {analysis['metrics'].character_accuracy * 100:.2f}%")
print(f"Word Accuracy: {analysis['metrics'].word_accuracy * 100:.2f}%")
```

## ğŸ“Š Output Format

```json
{
  "metrics": {
    "character_accuracy": 0.95,
    "word_accuracy": 0.92,
    "confidence_score": 0.88,
    "error_rate": 0.12,
    "yaml_validity": 0.85,
    "overall_quality": 89.5
  },
  "quality_grade": "A+ (Very Good - Minor fixes needed)",
  "recommendations": [
    {
      "category": "OCR Quality",
      "priority": "MEDIUM",
      "issue": "3å€‹ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º",
      "solution": "ç”»åƒã®å‰å‡¦ç†ã‚’å®Ÿæ–½",
      "expected_improvement": "+20-30% accuracy"
    }
  ]
}
```

## ğŸ“ˆ Success Metrics

- èª¤èªè­˜æ¤œå‡ºç‡: 95% ä»¥ä¸Š
- è‡ªå‹•è£œæ­£ç²¾åº¦: 90% ä»¥ä¸Š
- å‡¦ç†é€Ÿåº¦: <5ç§’/ãƒšãƒ¼ã‚¸
- YAMLç”ŸæˆæˆåŠŸç‡: 95% ä»¥ä¸Š

## ğŸ”— Integration Points

- **DeepSeek OCR Engine**: OCRã‚¨ãƒ³ã‚¸ãƒ³ã¨ã®çµ±åˆ
- **Image Preprocessor**: å‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ é€£æº
- **YAML Validator**: YAMLæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
- **Learning System**: èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’

---

**Version**: 1.0.0
**Last Updated**: 2025-11-05
**Maintainer**: OCR Quality Team
**Status**: Production Ready âœ…
